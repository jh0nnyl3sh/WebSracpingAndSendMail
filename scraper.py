import requests
from bs4 import BeautifulSoup
import csv
import time

# Fonksiyonumuzu tanÄ±mlÄ±yoruz
def get_adalet_news():
    url = "https://www.adalet.gov.tr/arsiv"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36"
    }
    
    print(f"ğŸ“¡ [SCRAPER] '{url}' adresine sÄ±zÄ±lÄ±yor...")
    time.sleep(1)
    
    response = requests.get(url, headers=headers)
    file_name = "adalet_duyurular.csv"
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        announcements = soup.find_all("a", class_="ab-announcement")
        
        with open(file_name, mode="w", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            writer.writerow(["SÄ±ra", "Duyuru BaÅŸlÄ±ÄŸÄ±", "Link"])
            
            count = 0
            for index, item in enumerate(announcements, 1):
                try:
                    href = item.get("href")
                    full_link = "https://www.adalet.gov.tr" + href if href and not href.startswith("http") else href
                    title_tag = item.find("h5")
                    title_text = title_tag.text.strip() if title_tag else item.text.strip()

                    if title_text:
                        writer.writerow([index, title_text, full_link])
                        count += 1
                except Exception:
                    continue
        
        print(f"âœ… [SCRAPER] {count} duyuru Ã§ekildi. Dosya hazÄ±r: {file_name}")
        # Ä°ÅTE KRÄ°TÄ°K NOKTA: DosyanÄ±n adÄ±nÄ± geri dÃ¶ndÃ¼rÃ¼yoruz (return)
        return file_name 
    else:
        print(f"âŒ [SCRAPER] BaÄŸlantÄ± hatasÄ±: {response.status_code}")
        return None # Hata varsa None dÃ¶ndÃ¼r